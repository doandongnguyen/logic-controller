{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Here, we search around [0., 1., 2., 3., 4., 5.], the step is 0.01\n",
    "> - [0., 1., 2., 3., 4., 5.] is a not bad one, we devote to improve it with tiny change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "import random\n",
    "\n",
    "class move_gym():\n",
    "    def __init__(self):\n",
    "        self.scope = 2.0\n",
    "        self.states = 6\n",
    "        self.actions = 4\n",
    "    def reset(self, s=[]):\n",
    "        if np.array( s ).shape[0] == 0:\n",
    "            self.obstacle_x = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "            self.obstacle_y = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "\n",
    "            self.move_x = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "            self.move_y = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1  \n",
    "            while ( abs(self.obstacle_x-self.move_x) < 1.05 ) & ( abs(self.obstacle_y-self.move_y) < 1.05 ):\n",
    "                self.move_x = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "                self.move_y = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1      \n",
    "\n",
    "            self.target_x = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "            self.target_y = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "            while (  ( abs(self.obstacle_x-self.target_x) < 1.05 ) & ( abs(self.obstacle_y-self.target_y) < 1.05 )  ) |\\\n",
    "             (  ( abs(self.move_x-self.target_x) < 1.05 ) & ( abs(self.move_y-self.target_y) < 1.05 )  ):\n",
    "                self.target_x = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1\n",
    "                self.target_y = np.random.randint( int(-self.scope*10), int(self.scope*10) )*0.1        \n",
    "        else:\n",
    "            self.obstacle_x=s[0]\n",
    "            self.obstacle_y=s[1]\n",
    "            self.target_x=s[2]\n",
    "            self.target_y=s[3]\n",
    "            self.move_x=s[4]\n",
    "            self.move_y=s[5]\n",
    "\n",
    "        state=np.array([ self.obstacle_x, self.obstacle_y, self.target_x, self.target_y, self.move_x, self.move_y ])\n",
    "        return state #, self.state2img(state)\n",
    "    def step(self, action):\n",
    "        velocity = 0.2\n",
    "        if action==0: # up down right left\n",
    "            self.move_y+=velocity\n",
    "        if action==1:\n",
    "            self.move_y-=velocity\n",
    "        if action==2:\n",
    "            self.move_x+=velocity\n",
    "        if action==3:\n",
    "            self.move_x-=velocity\n",
    "\n",
    "        if self.move_x > (self.scope+1.0):\n",
    "            self.move_x-=velocity\n",
    "        if self.move_x < (-self.scope-1.0):\n",
    "            self.move_x+=velocity\n",
    "        if self.move_y > (self.scope+1.0):\n",
    "            self.move_y-=velocity\n",
    "        if self.move_y < (-self.scope-1.0):\n",
    "            self.move_y+=velocity\n",
    "        \n",
    "        reward = -0.1\n",
    "        done = False\n",
    "        info = \"^_^\"\n",
    "        if (  ( abs(self.obstacle_x-self.move_x) < 1.05 ) & ( abs(self.obstacle_y-self.move_y) < 1.05 )  ): \n",
    "            reward = -1.0 \n",
    "            done = True\n",
    "            info = \"collision\"\n",
    "\n",
    "        elif (  ( abs(self.target_x-self.move_x) < 1.05 ) & ( abs(self.target_y-self.move_y) < 1.05 )  ):\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "            info = \"reach\"\n",
    "\n",
    "        state=np.array([ self.obstacle_x, self.obstacle_y, self.target_x, self.target_y, self.move_x, self.move_y ])\n",
    "\n",
    "        return state, reward,done,info\n",
    "    def state2img(self, state):\n",
    "        img = np.zeros([84,84,3], dtype=np.uint8)\n",
    "        ( img[ int(42-(self.obstacle_y+0.5)*10):int(42-(self.obstacle_y-0.5)*10), int((self.obstacle_x-0.5)*10+42):int((self.obstacle_x+0.5)*10+42),2 ] ).fill(255)\n",
    "        ( img[ int(42-(self.target_y+0.5)*10):int(42-(self.target_y-0.5)*10), int((self.target_x-0.5)*10+42):int((self.target_x+0.5)*10+42),1 ] ).fill(255)\n",
    "        ( img[ int(42-(self.move_y+0.5)*10):int(42-(self.move_y-0.5)*10), int((self.move_x-0.5)*10+42):int((self.move_x+0.5)*10+42),0 ] ).fill(255)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore old network weights ...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "\n",
    "class DQN():\n",
    "    # DQN Agent\n",
    "    def __init__(self, env):\n",
    "        # init experience replay\n",
    "        self.replay_buffer = deque()\n",
    "        # init. some parameters\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        \n",
    "        # self.state_dim = env.observation_space.shape[0]\n",
    "        self.state_dim = env.states\n",
    "\n",
    "        # self.action_dim = env.action_space.n\n",
    "        self.action_dim = env.actions\n",
    "\n",
    "        self.create_Q_network()\n",
    "        self.create_training_method()\n",
    "\n",
    "        # Init session\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def create_Q_network(self):\n",
    "        \n",
    "#         # # a: create new network weights\n",
    "#         print \"create new network weights ...\"\n",
    "#         num_hid = 200\n",
    "#         self.W1 = self.weight_variable([self.state_dim,num_hid])\n",
    "#         self.b1 = self.bias_variable([num_hid])\n",
    "#         self.W2 = self.weight_variable([num_hid,num_hid])\n",
    "#         self.b2 = self.bias_variable([num_hid])\n",
    "#         self.W3 = self.weight_variable([num_hid,self.action_dim])\n",
    "#         self.b3= self.bias_variable([self.action_dim])\n",
    "\n",
    "        # # b: restore old network weights\n",
    "        print \"restore old network weights ...\"\n",
    "        self.W1, self.b1, self.W2, self.b2, self.W3, self.b3 = self.restore_wb( \"train-3500/\" )\n",
    "\n",
    "\n",
    "        # input layer\n",
    "        self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n",
    "        # hidden layers\n",
    "        h_layer = tf.nn.relu(tf.matmul(self.state_input,self.W1) + self.b1)\n",
    "        h_layer02 = tf.nn.relu(tf.matmul(h_layer,self.W2) + self.b2)\n",
    "        # Q Value layer\n",
    "        self.Q_value = tf.matmul(h_layer02,self.W3) + self.b3\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot presentation\n",
    "        self.y_input = tf.placeholder(\"float\",[None])\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "        # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else :\n",
    "                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n",
    "\n",
    "        self.optimizer.run(feed_dict={\n",
    "          self.y_input:y_batch,\n",
    "          self.action_input:action_batch,\n",
    "          self.state_input:state_batch\n",
    "          })\n",
    "\n",
    "    def egreedy_action(self,state):\n",
    "        Q_value = self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]\n",
    "          })[0]\n",
    "        if random.random() <= 0.5:\n",
    "            return random.randint(0,self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "\n",
    "    def obtain_wb(self):\n",
    "        W1=self.session.run(self.W1)\n",
    "        b1=self.session.run(self.b1)\n",
    "        W2=self.session.run(self.W2)\n",
    "        b2=self.session.run(self.b2)\n",
    "        W3=self.session.run(self.W3)\n",
    "        b3=self.session.run(self.b3)\n",
    "        return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "    def restore_wb(self, directory):\n",
    "        W1 = np.load( directory+\"W1.npy\" )\n",
    "        W2 = np.load( directory+\"W2.npy\" )\n",
    "        W3 = np.load( directory+\"W3.npy\" )\n",
    "\n",
    "        b1 = np.load( directory+\"b1.npy\" )\n",
    "        b2 = np.load( directory+\"b2.npy\" )\n",
    "        b3 = np.load( directory+\"b3.npy\" )        \n",
    "        return tf.Variable(W1), tf.Variable(b1), tf.Variable(W2), tf.Variable(b2), tf.Variable(W3), tf.Variable(b3)\n",
    "    def use_nn(self,state):\n",
    "        return self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]\n",
    "          })\n",
    "\n",
    "    def action(self,state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict = {\n",
    "          self.state_input:[state]\n",
    "          })[0])\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    \n",
    "# Hyper Parameters\n",
    "STEP = 50 # Step limitation in an episode\n",
    "TEST = 10 # The number of experiment for test\n",
    "\n",
    "# initialize env and agent\n",
    "env = move_gym()\n",
    "agent = DQN(env)\n",
    "\n",
    "all_episode = 0\n",
    "all_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discrete_it(state, discrete):\n",
    "    \n",
    "    dis_s = []\n",
    "    for i in xrange( len(state) ):\n",
    "        num = state[i]\n",
    "        over = num - discrete\n",
    "        for j in xrange( len(over) ):\n",
    "            if over[j]<0:\n",
    "                over[j] = 100 # a certain big number\n",
    "        dis_s.append( np.argmin( over ) )\n",
    "    return dis_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def state2relative(state):\n",
    "    top_t = 100.0\n",
    "    bottom_t = 100.0\n",
    "    right_t = 100.0\n",
    "    left_t = 100.0\n",
    "\n",
    "    top_o = 100.0\n",
    "    bottom_o = 100.0\n",
    "    right_o = 100.0\n",
    "    left_o = 100.0        \n",
    "\n",
    "    obstacle_x=state[0]\n",
    "    obstacle_y=state[1]\n",
    "    target_x=state[2]\n",
    "    target_y=state[3]\n",
    "    move_x=state[4]\n",
    "    move_y=state[5]\n",
    "\n",
    "    if target_y >= move_y:\n",
    "        top_t = target_y - move_y\n",
    "    if target_y < move_y:\n",
    "        bottom_t = move_y - target_y\n",
    "    if target_x >= move_x:\n",
    "        right_t = target_x - move_x\n",
    "    if target_x < move_x:\n",
    "        left_t = move_x - target_x\n",
    "\n",
    "    if obstacle_y >= move_y:\n",
    "        top_o = obstacle_y - move_y\n",
    "    if obstacle_y < move_y:\n",
    "        bottom_o = move_y - obstacle_y\n",
    "    if obstacle_x >= move_x:\n",
    "        right_o = obstacle_x - move_x\n",
    "    if obstacle_x < move_x:\n",
    "        left_o = move_x - obstacle_x\n",
    "\n",
    "    return np.array([ top_t, bottom_t, right_t, left_t, top_o, bottom_o, right_o, left_o ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logic_create(discrete):\n",
    "    logic_q = np.zeros( [6,6,6,6, 6,6,6,6, 5] )\n",
    "    \n",
    "    for it_tms in xrange(10):\n",
    "        info =\"^_^\"\n",
    "        good_state = False\n",
    "        for i in xrange(3000):\n",
    "            if (info==\"reach\") & (good_state == False):\n",
    "                state = env.reset(A)\n",
    "#                 C.append(A)\n",
    "                good_state = True\n",
    "            else:\n",
    "                state = env.reset()\n",
    "                good_state = False\n",
    "            A = state\n",
    "            for j in xrange(STEP):\n",
    "                relative_state=state2relative(state)\n",
    "                # discrete it:\n",
    "                relative_state = discrete_it( relative_state, discrete )\n",
    "\n",
    "                action = agent.action(state)\n",
    "                if good_state:\n",
    "                    if max(logic_q[relative_state[0]][relative_state[1]][relative_state[2]][relative_state[3]]\\\n",
    "                        [relative_state[4]][relative_state[5]][relative_state[6]][relative_state[7]])<1000:\n",
    "\n",
    "                        logic_q[relative_state[0]][relative_state[1]][relative_state[2]][relative_state[3]]\\\n",
    "                        [relative_state[4]][relative_state[5]][relative_state[6]][relative_state[7]][action+1] +=1\n",
    "\n",
    "                state, reward,done,info = env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        ###############################\n",
    "        if it_tms==9:\n",
    "            B = 0\n",
    "            B11= 0\n",
    "\n",
    "            for i in xrange(1000):\n",
    "                state = env.reset()\n",
    "                A = state\n",
    "                for j in xrange(STEP):\n",
    "                    relative_state=state2relative(state)\n",
    "                    # discrete it:\n",
    "                    relative_state = discrete_it( relative_state, discrete )\n",
    "\n",
    "                    action = np.argmax( logic_q[relative_state[0]][relative_state[1]][relative_state[2]][relative_state[3]]\\\n",
    "                    [relative_state[4]][relative_state[5]][relative_state[6]][relative_state[7]] )-1\n",
    "                    state, reward,done,info = env.step(action)\n",
    "                    if done:\n",
    "                        if info==\"collision\":\n",
    "#                             B1.append(A)\n",
    "#                             B11.append(A)\n",
    "                                B11 += 1\n",
    "                        break\n",
    "                if done == 0:\n",
    "#                     B.append(A)\n",
    "                        B += 1\n",
    "\n",
    "        ###############################\n",
    "    \n",
    "#             print \"len(B): \", len(B)\n",
    "#             print \"len(B11): \", len(B11)\n",
    "#             print \"len(B1): \", len(B1)\n",
    "#             np.save(\"logic_q-only\", logic_q)\n",
    "#             np.save(\"C\", C)\n",
    "\n",
    "    return B, B11 # len(B), len(B11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>:  0 22 11 [ 0.    1.1   1.83  2.88  4.18  5.08]\n",
      ">>:  1 38 79 [ 0.    0.95  1.8   3.07  3.97  4.9 ]\n",
      ">>:  2 18 82 [ 0.    0.95  1.98  2.92  4.01  4.88]\n",
      ">>:  3 23 175 [ 0.    0.88  1.82  2.94  4.22  5.04]\n",
      ">>:  4 142 101 [ 0.    1.13  1.95  3.04  4.11  4.89]\n",
      ">>:  5 26 83 [ 0.    0.97  1.87  2.88  4.01  4.98]\n",
      ">>:  6 22 165 [ 0.    0.88  1.93  2.86  4.11  4.93]\n",
      ">>:  7 28 4 [ 0.    1.04  1.76  2.84  4.24  5.04]\n",
      ">>:  8 155 68 [ 0.    1.11  1.95  2.82  4.14  5.05]\n",
      ">>:  9 26 10 [ 0.    1.06  1.77  2.98  4.19  5.14]\n",
      ">>:  10 20 167 [ 0.    0.88  1.9   2.96  4.16  5.1 ]\n",
      ">>:  11 21 76 [ 0.    0.98  1.97  2.8   4.08  5.17]\n",
      ">>:  12 29 8 [ 0.    1.03  1.8   3.05  4.18  4.9 ]\n",
      ">>:  13 14 95 [ 0.    0.94  1.89  2.84  3.97  5.01]\n",
      ">>:  14 162 96 [ 0.    1.11  1.91  2.78  3.98  5.11]\n",
      ">>:  15 28 79 [ 0.    0.95  1.73  2.89  4.16  5.06]\n",
      ">>:  16 174 115 [ 0.    1.17  1.92  3.08  4.06  5.12]\n",
      ">>:  17 22 163 [ 0.    0.89  1.7   3.    4.18  5.01]\n",
      ">>:  18 172 85 [ 0.    1.11  1.91  3.06  4.03  4.88]\n",
      ">>:  19 19 163 [ 0.    0.87  1.97  2.82  4.08  5.08]\n",
      ">>:  20 193 60 [ 0.    1.12  1.78  2.82  4.17  5.17]\n",
      ">>:  21 35 13 [ 0.    1.02  1.94  3.01  4.15  4.95]\n",
      ">>:  22 27 8 [ 0.    1.08  1.93  3.04  4.14  5.03]\n",
      ">>:  23 31 7 [ 0.    1.09  1.75  2.9   4.04  5.16]\n",
      ">>:  24 31 5 [ 0.    1.07  1.76  2.94  4.14  4.89]\n",
      ">>:  25 28 85 [ 0.    0.92  1.7   2.95  4.12  5.07]\n",
      ">>:  26 180 92 [ 0.    1.11  1.93  3.    4.15  4.96]\n",
      ">>:  27 19 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  28 26 104 [ 0.    0.99  1.94  2.77  4.24  5.11]\n",
      ">>:  29 30 78 [ 0.    0.91  1.89  2.92  4.07  5.11]\n",
      ">>:  30 157 77 [ 0.    1.15  1.91  2.9   4.11  4.95]\n",
      ">>:  31 159 94 [ 0.    1.17  1.83  2.83  4.16  4.96]\n",
      ">>:  32 26 82 [ 0.    0.91  1.97  2.94  4.01  5.05]\n",
      ">>:  33 28 90 [ 0.    0.94  1.79  3.08  4.25  5.08]\n",
      ">>:  34 42 8 [ 0.    1.05  1.79  2.88  4.02  4.94]\n",
      ">>:  35 32 50 [ 0.    1.    1.76  3.04  4.19  5.16]\n",
      ">>:  36 162 71 [ 0.    1.13  1.81  2.99  4.11  4.9 ]\n",
      ">>:  37 29 84 [ 0.    0.93  1.94  3.01  4.06  4.97]\n",
      ">>:  38 179 95 [ 0.    1.12  1.83  3.07  4.11  5.06]\n",
      ">>:  39 21 103 [ 0.    0.93  1.74  2.96  4.05  4.97]\n",
      ">>:  40 37 4 [ 0.    1.05  1.72  3.06  4.09  5.11]\n",
      ">>:  41 19 10 [ 0.    1.04  1.9   2.92  3.97  5.07]\n",
      ">>:  42 25 3 [ 0.    1.07  1.81  3.    3.94  5.01]\n",
      ">>:  43 31 6 [ 0.    1.03  1.78  2.8   4.23  4.97]\n",
      ">>:  44 23 110 [ 0.    0.95  1.93  2.8   4.    5.03]\n",
      ">>:  45 25 169 [ 0.    0.88  1.8   2.99  4.09  5.18]\n",
      ">>:  46 20 131 [ 0.    0.9   1.7   3.02  4.19  5.15]\n",
      ">>:  47 164 82 [ 0.    1.13  1.93  2.82  4.22  5.05]\n",
      ">>:  48 20 127 [ 0.    0.9   1.89  3.02  4.12  5.08]\n",
      ">>:  49 38 185 [ 0.    0.86  1.71  3.02  4.02  5.08]\n",
      "Gen. 0 (0.00%): Max/Min/Avg Fitness(Raw) [1035.29(974.00)/627.41(711.00)/862.74(862.74)]\n",
      ">>:  50 29 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  51 25 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  52 18 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  53 24 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  54 20 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  55 26 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  56 24 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  57 23 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  58 25 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  59 27 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  60 20 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  61 37 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  62 25 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  63 19 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  64 23 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  65 25 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  66 24 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  67 18 11 [ 0.    1.06  1.87  2.86  4.24  4.99]\n",
      ">>:  68 24 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  69 17 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  70 33 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  71 42 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  72 24 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  73 30 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  74 27 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  75 22 5 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  76 33 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  77 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  78 20 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  79 29 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  80 194 102 [ 0.    1.16  1.9   2.86  4.11  4.99]\n",
      ">>:  81 32 11 [ 0.    1.06  1.87  2.86  4.24  4.99]\n",
      ">>:  82 27 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  83 19 4 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  84 22 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  85 29 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  86 22 11 [ 0.    1.03  2.    2.86  4.14  4.99]\n",
      ">>:  87 28 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  88 23 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  89 25 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  90 21 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  91 29 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  92 26 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  93 33 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  94 20 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  95 31 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  96 16 24 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  97 16 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  98 30 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  99 29 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  100 18 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  101 18 18 [ 0.    1.06  2.    2.86  4.06  5.04]\n",
      ">>:  102 31 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  103 21 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  104 22 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  105 27 11 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  106 27 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  107 27 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  108 20 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  109 29 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  110 29 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  111 22 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  112 22 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  113 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  114 32 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  115 22 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  116 29 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  117 183 91 [ 0.    1.16  1.9   2.86  4.11  4.99]\n",
      ">>:  118 23 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  119 32 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  120 21 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  121 22 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  122 20 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  123 22 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  124 24 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  125 20 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  126 21 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  127 25 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  128 24 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  129 31 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  130 22 24 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  131 20 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  132 21 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  133 34 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  134 22 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  135 19 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  136 37 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  137 24 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  138 24 7 [ 0.    1.06  1.79  3.07  4.11  4.99]\n",
      ">>:  139 23 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  140 22 3 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  141 11 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  142 23 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  143 13 80 [ 0.    0.95  2.    2.97  4.11  4.99]\n",
      ">>:  144 26 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  145 24 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  146 20 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  147 16 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  148 28 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  149 31 28 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  150 25 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  151 15 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  152 18 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  153 21 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  154 24 24 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  155 26 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  156 17 14 [ 0.    1.06  1.87  2.86  4.24  4.99]\n",
      ">>:  157 23 8 [ 0.    1.06  2.    2.86  4.06  5.04]\n",
      ">>:  158 26 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  159 21 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  160 19 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  161 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  162 31 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  163 32 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  164 30 26 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  165 16 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  166 20 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  167 17 18 [ 0.    1.06  2.    2.94  4.03  4.99]\n",
      ">>:  168 22 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  169 18 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  170 26 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  171 27 3 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  172 14 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  173 26 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  174 20 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  175 25 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  176 23 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  177 19 96 [ 0.    0.95  2.    2.97  4.11  4.99]\n",
      ">>:  178 27 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  179 21 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  180 24 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  181 25 95 [ 0.    0.95  2.    2.97  4.11  4.99]\n",
      ">>:  182 21 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  183 23 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  184 26 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  185 32 27 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  186 22 14 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  187 20 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  188 17 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  189 22 17 [ 0.    1.06  2.    2.86  4.06  5.04]\n",
      ">>:  190 25 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  191 26 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  192 28 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  193 28 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  194 24 31 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  195 11 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  196 16 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  197 19 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  198 24 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  199 26 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  200 29 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  201 18 24 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  202 19 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  203 28 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  204 23 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  205 25 24 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  206 20 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  207 28 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  208 25 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  209 22 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  210 18 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  211 27 10 [ 0.    1.06  1.82  2.86  4.11  5.17]\n",
      ">>:  212 17 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  213 26 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  214 18 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  215 22 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  216 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  217 23 110 [ 0.    0.95  2.    2.97  4.11  4.99]\n",
      ">>:  218 23 12 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  219 19 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  220 26 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  221 29 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  222 17 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  223 19 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  224 17 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  225 27 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  226 19 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  227 32 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  228 25 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  229 18 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  230 28 4 [ 0.    1.03  2.    2.86  4.14  4.99]\n",
      ">>:  231 21 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  232 24 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  233 21 27 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  234 19 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  235 19 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  236 21 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  237 28 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  238 24 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  239 25 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  240 25 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  241 17 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  242 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  243 24 12 [ 0.    1.03  2.    2.86  4.14  4.99]\n",
      ">>:  244 27 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  245 37 5 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  246 19 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  247 24 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  248 28 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  249 24 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  250 20 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  251 26 11 [ 0.    1.03  2.    2.86  4.14  4.99]\n",
      ">>:  252 34 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  253 28 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  254 20 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  255 24 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  256 36 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  257 20 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  258 23 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  259 43 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  260 30 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  261 24 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  262 20 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  263 28 27 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  264 17 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  265 22 32 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  266 21 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  267 23 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  268 26 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  269 26 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  270 20 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  271 32 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  272 22 4 [ 0.    1.03  1.82  2.86  4.14  5.17]\n",
      ">>:  273 22 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  274 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  275 27 8 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  276 18 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  277 28 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  278 20 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  279 15 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  280 21 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  281 32 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  282 30 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  283 16 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  284 21 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  285 27 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  286 19 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  287 22 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  288 21 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  289 25 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  290 28 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  291 22 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  292 27 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  293 26 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  294 26 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  295 29 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  296 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  297 20 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  298 22 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  299 28 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  300 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  301 24 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  302 15 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  303 20 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  304 29 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  305 20 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  306 16 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  307 27 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  308 21 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  309 24 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  310 23 4 [ 0.    1.06  1.82  2.86  4.11  5.17]\n",
      ">>:  311 17 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  312 22 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  313 23 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  314 25 14 [ 0.    1.06  2.    2.94  4.03  4.99]\n",
      ">>:  315 31 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  316 18 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  317 30 19 [ 0.    1.06  1.82  2.86  4.11  5.17]\n",
      ">>:  318 23 11 [ 0.    1.06  2.    2.86  4.06  5.04]\n",
      ">>:  319 25 27 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  320 31 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  321 20 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  322 22 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  323 23 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  324 21 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  325 18 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  326 26 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  327 26 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  328 32 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  329 30 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  330 31 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  331 21 10 [ 0.    1.03  2.    2.86  4.14  4.99]\n",
      ">>:  332 23 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  333 20 11 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  334 19 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  335 25 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  336 25 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  337 25 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  338 25 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  339 24 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  340 31 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  341 19 2 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  342 31 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  343 19 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  344 25 17 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  345 18 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  346 25 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  347 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  348 35 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  349 36 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  350 20 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  351 26 8 [ 0.    1.06  1.79  3.07  4.11  4.99]\n",
      ">>:  352 24 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  353 20 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  354 24 4 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  355 17 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  356 28 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  357 26 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  358 20 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  359 23 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  360 22 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  361 35 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  362 25 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  363 24 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  364 21 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  365 33 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  366 23 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  367 21 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  368 17 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  369 28 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  370 21 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  371 15 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  372 27 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  373 31 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  374 28 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  375 24 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  376 24 90 [ 0.    0.98  2.    2.86  4.11  5.07]\n",
      ">>:  377 28 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  378 19 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  379 29 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  380 15 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  381 25 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  382 23 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  383 23 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  384 25 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  385 19 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  386 20 5 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  387 26 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  388 25 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  389 30 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  390 30 23 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  391 34 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  392 26 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  393 30 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  394 29 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  395 24 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  396 16 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  397 23 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  398 32 3 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  399 25 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  400 23 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  401 28 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  402 26 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  403 24 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  404 26 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  405 19 26 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  406 24 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  407 19 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  408 22 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  409 22 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  410 29 27 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  411 28 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  412 20 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  413 18 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  414 23 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  415 37 5 [ 0.    1.06  1.79  3.07  4.11  4.99]\n",
      ">>:  416 24 12 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  417 23 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  418 18 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  419 29 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  420 21 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  421 29 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  422 26 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  423 23 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  424 32 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  425 31 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  426 18 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  427 32 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  428 20 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  429 27 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  430 148 93 [ 0.    1.16  1.9   2.86  4.11  4.99]\n",
      ">>:  431 25 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  432 27 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  433 28 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  434 25 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  435 24 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  436 25 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  437 18 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  438 27 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  439 25 7 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  440 17 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  441 16 18 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  442 13 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  443 22 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  444 25 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  445 20 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  446 20 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  447 19 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  448 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  449 17 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  450 13 31 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  451 26 91 [ 0.    0.98  2.    2.86  4.11  5.07]\n",
      ">>:  452 21 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  453 28 25 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  454 27 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  455 27 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  456 30 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  457 22 89 [ 0.    0.98  2.    2.86  4.11  5.07]\n",
      ">>:  458 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  459 25 10 [ 0.    1.06  2.    2.89  4.11  4.96]\n",
      ">>:  460 20 9 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  461 21 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  462 27 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  463 25 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  464 32 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  465 30 9 [ 0.    1.06  1.87  2.86  4.24  4.99]\n",
      ">>:  466 30 10 [ 0.    1.06  1.87  2.86  4.24  4.99]\n",
      ">>:  467 30 14 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  468 26 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  469 17 13 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  470 22 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  471 22 22 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  472 31 24 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  473 13 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  474 19 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  475 33 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  476 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  477 24 15 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  478 27 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  479 19 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  480 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  481 26 6 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  482 28 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  483 27 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  484 20 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  485 29 19 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  486 23 27 [ 0.    1.06  2.    2.94  4.06  4.96]\n",
      ">>:  487 17 26 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  488 23 17 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  489 22 21 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  490 26 10 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  491 26 20 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  492 24 8 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  493 29 3 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  494 29 102 [ 0.    0.98  2.    2.86  4.11  5.07]\n",
      ">>:  495 32 12 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  496 25 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  497 25 16 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  498 21 11 [ 0.    1.06  2.    2.86  4.11  4.99]\n",
      ">>:  499 20 8 [ 0.    1.06  1.87  2.86  4.24  4.99]\n",
      "Gen. 9 (100.00%): Max/Min/Avg Fitness(Raw) [1150.39(979.00)/245.46(883.00)/958.66(958.66)]\n",
      "Total time elapsed: 81137.764 seconds.\n",
      "- GenomeBase\n",
      "\tScore:\t\t\t 979.000000\n",
      "\tFitness:\t\t 1150.392000\n",
      "\n",
      "\tParams:\t\t {'rangemax': 31, 'rangemin': 0}\n",
      "\n",
      "\tSlot [Evaluator] (Count: 1)\n",
      "\t\tName: eval_func - Weight: 0.50\n",
      "\tSlot [Initializator] (Count: 1)\n",
      "\t\tName: G1DListInitializatorInteger - Weight: 0.50\n",
      "\t\tDoc:  Integer initialization function of G1DList\n",
      "\n",
      "   This initializator accepts the *rangemin* and *rangemax* genome parameters.\n",
      "\n",
      "   \n",
      "\tSlot [Mutator] (Count: 1)\n",
      "\t\tName: G1DListMutatorSwap - Weight: 0.50\n",
      "\t\tDoc:  The mutator of G1DList, Swap Mutator\n",
      "   \n",
      "   .. note:: this mutator is :term:`Data Type Independent`\n",
      "\n",
      "   \n",
      "\tSlot [Crossover] (Count: 1)\n",
      "\t\tName: G1DListCrossoverSinglePoint - Weight: 0.50\n",
      "\t\tDoc:  The crossover of G1DList, Single Point\n",
      "\n",
      "   .. warning:: You can't use this crossover method for lists with just one element.\n",
      "\n",
      "   \n",
      "\n",
      "- G1DList\n",
      "\tList size:\t 5\n",
      "\tList:\t\t [20, 30, 9, 17, 12]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyevolve import G1DList\n",
    "\n",
    "from pyevolve import GSimpleGA\n",
    "\n",
    "\n",
    "# This function is the evaluation function, we want\n",
    "\n",
    "# to give high score to more zero'ed chromosomes\n",
    "\n",
    "ink = 0\n",
    "\n",
    "def eval_func(chromosome):\n",
    "    global ink\n",
    "    A=[]\n",
    "    for value in chromosome:\n",
    "        A.append( value )\n",
    "    discrete = np.array( [ 0.,    1.02,  1.86,  2.93,  4.1,   5.03] ) # if only discrete[0] = 0.0\n",
    "\n",
    "    for i in range(5):\n",
    "#         discrete[i+1] = discrete[i] + A[i]*0.05\n",
    "        discrete[i+1] = discrete[i+1] + ( A[i]-16 )*0.01\n",
    "    \n",
    "    \n",
    "    n_no_done, n_collision = logic_create(discrete)\n",
    "    \n",
    "    print \">>: \", ink, n_no_done, n_collision, discrete\n",
    "    ink += 1\n",
    "\n",
    "    return 1000 - n_no_done - n_collision\n",
    "    \n",
    "#    score = 0.0\n",
    "\n",
    "#    # iterate over the chromosome elements (items)\n",
    "\n",
    "#    for value in chromosome:\n",
    "\n",
    "#       if value==0:\n",
    "\n",
    "#          score += 1.0\n",
    "\n",
    "#    return score+1000\n",
    "\n",
    "\n",
    "# Genome instance\n",
    "\n",
    "genome = G1DList.G1DList(5)\n",
    "\n",
    "genome.setParams(rangemin=0, rangemax=31)\n",
    "\n",
    "# The evaluator function (objective function)\n",
    "\n",
    "genome.evaluator.set(eval_func)\n",
    "\n",
    "ga = GSimpleGA.GSimpleGA(genome)\n",
    "\n",
    "ga.setGenerations(9)\n",
    "ga.setPopulationSize(50)\n",
    "\n",
    "# Do the evolution, with stats dump\n",
    "\n",
    "# frequency of 10 generations\n",
    "\n",
    "ga.evolve(freq_stats=10)\n",
    "\n",
    "\n",
    "# Best individual\n",
    "\n",
    "print ga.bestIndividual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "> 24 62 [ 0.  1.  2.  3.  4.  5.]  \n",
    "75 30 4 [ 0.    1.02  1.86  2.93  4.1   5.03]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
